
@misc{wu_predicting_2021,
	title = {Predicting the {Reproducibility} of {Social} and {Behavioral} {Science} {Papers} {Using} {Supervised} {Learning} {Models}},
	url = {http://arxiv.org/abs/2104.04580},
	doi = {10.48550/arXiv.2104.04580},
	abstract = {In recent years, significant effort has been invested verifying the reproducibility and robustness of research claims in social and behavioral sciences (SBS), much of which has involved resource-intensive replication projects. In this paper, we investigate prediction of the reproducibility of SBS papers using machine learning methods based on a set of features. We propose a framework that extracts five types of features from scholarly work that can be used to support assessments of reproducibility of published research claims. Bibliometric features, venue features, and author features are collected from public APIs or extracted using open source machine learning libraries with customized parsers. Statistical features, such as p-values, are extracted by recognizing patterns in the body text. Semantic features, such as funding information, are obtained from public APIs or are extracted using natural language processing models. We analyze pairwise correlations between individual features and their importance for predicting a set of human-assessed ground truth labels. In doing so, we identify a subset of 9 top features that play relatively more important roles in predicting the reproducibility of SBS papers in our corpus. Results are verified by comparing performances of 10 supervised predictive classifiers trained on different sets of features.},
	language = {en},
	urldate = {2025-09-23},
	publisher = {arXiv},
	author = {Wu, Jian and Nivargi, Rajal and Lanka, Sree Sai Teja and Menon, Arjun Manoj and Modukuri, Sai Ajay and Nakshatri, Nishanth and Wei, Xin and Wang, Zhuoer and Caverlee, James and Rajtmajer, Sarah M. and Giles, C. Lee},
	month = oct,
	year = {2021},
	note = {arXiv:2104.04580 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Digital Libraries, Computer Science - Machine Learning},
	annote = {Comment: 17 pages, 8 figures},
	file = {PDF:/home/jkr/Zotero/storage/PA5WTUGY/Wu et al. - 2021 - Predicting the Reproducibility of Social and Behavioral Science Papers Using Supervised Learning Mod.pdf:application/pdf},
}

@misc{alipourfard_systematizing_2021,
	title = {Systematizing {Confidence} in {Open} {Research} and {Evidence} ({SCORE})},
	url = {https://osf.io/46mnb_v1},
	doi = {10.31235/osf.io/46mnb},
	abstract = {Assessing the credibility of research claims is a central, continuous, and laborious part of the scientific process. Credibility assessment strategies range from expert judgment to aggregating existing evidence to systematic replication efforts. Such assessments can require substantial time and effort. Research progress could be accelerated if there were rapid, scalable, accurate credibility indicators to guide attention and resource allocation for further assessment. The SCORE program is creating and validating algorithms to provide confidence scores for research claims at scale. To investigate the viability of scalable tools, teams are creating: a database of claims from papers in the social and behavioral sciences; expert and machine generated estimates of credibility; and, evidence of reproducibility, robustness, and replicability to validate the estimates. Beyond the primary research objective, the data and artifacts generated from this program will be openly shared and provide an unprecedented opportunity to examine research credibility and evidence.},
	language = {en-us},
	urldate = {2025-09-23},
	publisher = {OSF},
	author = {Alipourfard, Nazanin and Arendt, Beatrix and Benjamin, Daniel and Benkler, Noam and Bishop, Michael and Burstein, Mark and Bush, Martin and Caverlee, James and Chen, Yiling and Clark, Chae and Dreber, Anna and Errington, Timothy and Fidler, Fiona and Field, Samuel and Fox, Nicholas and Frank, Aaron and Fraser, Hannah and Friedman, Scott and Gelman, Ben and Gentile, James and Giles, C. and Gordon, Michael and Gordon-Sarney, Reed and Griffin, Christopher and Gulden, Timothy and Hahn, Krystal and Hartman, Robert and Holzmeister, Felix and Hu, Xia and Johannesson, Magnus and Kezar, Lee and Kline Struhl, Melissa and Kuter, Ugur and Kwasnica, Anthony and Lee, Dong-Ho and Lerman, Kristina and Liu, Yang and Loomas, Zachary and Luis, Brianna and Magnusson, Ian and Miske, Olivia and Mody, Fallon and Morstatter, Fred and Nosek, Brian and Parsons, Elan and Pennock, David and Pfeiffer, Thomas and Pujara, Jay and Rajtmajer, Sarah and Ren, Xiang and Salinas, Abel and Selvam, Ravi Kiran and Shipman, Frank and Silverstein, Priya and Sprenger, Amber and Squicciarini, Anna and Stratman, Steve and Sun, Kexuan and Tikoo, Saatvik and Twardy, Charles and Tyner, Andrew and Viganola, Domenico and Wang, Juntao and Wilkinson, David and Wintle, Bonnie and Wu, Jian},
	month = may,
	year = {2021},
	keywords = {algorithms, credibility, Metascience, replicability, reproducibility, social sciences},
	file = {OSF Preprint:/home/jkr/Zotero/storage/SHKVIAAT/Alipourfard et al. - 2021 - Systematizing Confidence in Open Research and Evidence (SCORE).pdf:application/pdf},
}

@misc{noauthor_systematizing_nodate,
	title = {Systematizing {Confidence} in {Open} {Research} and {Evidence}},
	url = {https://www.darpa.mil/research/programs/systematizing-confidence-in-open-research-and-evidence},
	urldate = {2025-09-23},
}

@inproceedings{lopez_mining_2021,
	address = {New York, NY, USA},
	series = {{CIKM} '21},
	title = {Mining {Software} {Entities} in {Scientific} {Literature}: {Document}-level {NER} for an {Extremely} {Imbalance} and {Large}-scale {Task}},
	isbn = {978-1-4503-8446-9},
	shorttitle = {Mining {Software} {Entities} in {Scientific} {Literature}},
	url = {https://dl.acm.org/doi/10.1145/3459637.3481936},
	doi = {10.1145/3459637.3481936},
	abstract = {We present a comprehensive information extraction system dedicated to software entities in scientific literature. This task combines the complexity of automatic reading of scientific documents (PDF processing, document structuring, styled/rich text, scaling) with challenges specific to mining software entities: high heterogeneity and extreme sparsity of mentions, document-level cross-references, disambiguation of noisy software mentions and poor portability of Machine Learning approaches between highly specialized domains. While NER is a key component to recognize new and unseen software, considering this task as a simple NER application fails to address most of these issues.In this paper, we propose a multi-model Machine Learning approach where raw documents are ingested by a cascade of document structuring processes applied not to text, but to layout token elements. The cascading process further enriches the relevant structures of the document with a Deep Learning software mention recognizer adapted to the high sparsity of mentions. The Machine Learning cascade culminates with entity disambiguation to alleviate false positives and to provide software entity linking. A bibliographical reference resolution is integrated to the process for attaching references cited alongside the software mentions.Based on the first gold-standard annotated dataset developed for software mentions, this work establishes a new reference end-to-end performance for this task. Experiments with the CORD-19 publications have further demonstrated that our system provides practically usable performance and is scalable to the whole scientific corpus, enabling novel applications for crediting research software and for better understanding the impact of software in science.},
	urldate = {2025-09-29},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Lopez, Patrice and Du, Caifan and Cohoon, Johanna and Ram, Karthik and Howison, James},
	month = oct,
	year = {2021},
	pages = {3986--3995},
	file = {Full Text PDF:/home/jkr/Zotero/storage/XWNBBIF7/Lopez et al. - 2021 - Mining Software Entities in Scientific Literature Document-level NER for an Extremely Imbalance and.pdf:application/pdf},
}

@article{youyou_discipline-wide_2023,
	title = {A discipline-wide investigation of the replicability of {Psychology} papers over the past two decades},
	volume = {120},
	url = {https://www.pnas.org/doi/10.1073/pnas.2208863120},
	doi = {10.1073/pnas.2208863120},
	abstract = {Conjecture about the weak replicability in social sciences has made scholars eager to quantify the scale and scope of replication failure for a discipline. Yet small-scale manual replication methods alone are ill-suited to deal with this big data problem. Here, we conduct a discipline-wide replication census in science. Our sample (N = 14,126 papers) covers nearly all papers published in the six top-tier Psychology journals over the past 20 y. Using a validated machine learning model that estimates a paper’s likelihood of replication, we found evidence that both supports and refutes speculations drawn from a relatively small sample of manual replications. First, we find that a single overall replication rate of Psychology poorly captures the varying degree of replicability among subfields. Second, we find that replication rates are strongly correlated with research methods in all subfields. Experiments replicate at a significantly lower rate than do non-experimental studies. Third, we find that authors’ cumulative publication number and citation impact are positively related to the likelihood of replication, while other proxies of research quality and rigor, such as an author’s university prestige and a paper’s citations, are unrelated to replicability. Finally, contrary to the ideal that media attention should cover replicable research, we find that media attention is positively related to the likelihood of replication failure. Our assessments of the scale and scope of replicability are important next steps toward broadly resolving issues of replicability.},
	number = {6},
	urldate = {2025-09-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Youyou, Wu and Yang, Yang and Uzzi, Brian},
	month = feb,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2208863120},
	file = {Full Text PDF:/home/jkr/Zotero/storage/MV9P2DT2/Youyou et al. - 2023 - A discipline-wide investigation of the replicability of Psychology papers over the past two decades.pdf:application/pdf},
}

@misc{priem_openalex_2022,
	title = {{OpenAlex}: {A} fully-open index of scholarly works, authors, venues, institutions, and concepts},
	shorttitle = {{OpenAlex}},
	url = {http://arxiv.org/abs/2205.01833},
	doi = {10.48550/arXiv.2205.01833},
	abstract = {OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Priem, Jason and Piwowar, Heather and Orr, Richard},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01833 [cs]},
	keywords = {Computer Science - Digital Libraries},
	annote = {Comment: Submitted to the 26th International Conference on Science, Technology and Innovation Indicators (STI 2022)},
	file = {Preprint PDF:/home/jkr/Zotero/storage/CWWTYMAQ/Priem et al. - 2022 - OpenAlex A fully-open index of scholarly works, authors, venues, institutions, and concepts.pdf:application/pdf;Snapshot:/home/jkr/Zotero/storage/H5BZIXSV/2205.html:text/html},
}

@misc{noauthor_academic_nodate,
	title = {Academic {Job} {Posting} {Service}},
	url = {https://www.hochschulverband.de/en/services/academic-job-posting-service},
	urldate = {2025-11-17},
}

@misc{GROBID,
    title = {GROBID},
    howpublished = {\url{https://github.com/kermitt2/grobid}},
    publisher = {GitHub},
    year = {2008--2025},
    archivePrefix = {swh},
    eprint = {1:dir:dab86b296e3c3216e2241968f0d63b68e8209d3c}
}